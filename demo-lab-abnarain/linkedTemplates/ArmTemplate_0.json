{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"factoryName": {
			"type": "string",
			"metadata": "Data Factory Name",
			"defaultValue": "demo-lab-abnarain"
		},
		"AmazonS31_secretAccessKey": {
			"type": "secureString",
			"metadata": "Secure string for 'secretAccessKey' of 'AmazonS31'"
		},
		"AzureDataLakeStorageGen2_ls_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorageGen2_ls'"
		},
		"AzureDatabricks_LS_accessToken": {
			"type": "secureString",
			"metadata": "Secure string for 'accessToken' of 'AzureDatabricks_LS'"
		},
		"AzureDatabricks_jobCluster_LS_accessToken": {
			"type": "secureString",
			"metadata": "Secure string for 'accessToken' of 'AzureDatabricks_jobCluster_LS'"
		},
		"hdiOnDemand_storage_ls_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'hdiOnDemand_storage_ls'"
		},
		"sinkBlob_LS_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'sinkBlob_LS'"
		},
		"OnDemandHDInsightConnection_clusterPassword": {
			"type": "secureString",
			"metadata": "Secure string for 'clusterPassword' of 'OnDemandHDInsightConnection'"
		},
		"OnDemandHDInsightConnection_clusterSshPassword": {
			"type": "secureString",
			"metadata": "Secure string for 'clusterSshPassword' of 'OnDemandHDInsightConnection'"
		},
		"OnDemandHDInsightConnection_servicePrincipalKey": {
			"type": "secureString",
			"metadata": "Secure string for 'servicePrincipalKey' of 'OnDemandHDInsightConnection'"
		},
		"hdiOnDemand_ls_clusterPassword": {
			"type": "secureString",
			"metadata": "Secure string for 'clusterPassword' of 'hdiOnDemand_ls'"
		},
		"hdiOnDemand_ls_clusterSshPassword": {
			"type": "secureString",
			"metadata": "Secure string for 'clusterSshPassword' of 'hdiOnDemand_ls'"
		},
		"hdiOnDemand_ls_servicePrincipalKey": {
			"type": "secureString",
			"metadata": "Secure string for 'servicePrincipalKey' of 'hdiOnDemand_ls'"
		},
		"linkedService1_password": {
			"type": "secureString",
			"metadata": "Secure string for 'password' of 'linkedService1'"
		},
		"linkedService2_clusterPassword": {
			"type": "secureString",
			"metadata": "Secure string for 'clusterPassword' of 'linkedService2'"
		},
		"linkedService2_clusterSshPassword": {
			"type": "secureString",
			"metadata": "Secure string for 'clusterSshPassword' of 'linkedService2'"
		},
		"linkedService2_servicePrincipalKey": {
			"type": "secureString",
			"metadata": "Secure string for 'servicePrincipalKey' of 'linkedService2'"
		},
		"linkedService_hdinsightBYOC_password": {
			"type": "secureString",
			"metadata": "Secure string for 'password' of 'linkedService_hdinsightBYOC'"
		},
		"AmazonS31_properties_typeProperties_accessKeyId": {
			"type": "string",
			"defaultValue": "AKIAI7PF6C2LJQ2X75VA"
		},
		"AzureDataLakeStorageGen2_ls_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://abnarainadlsgen2sink.dfs.core.windows.net"
		},
		"DataDestinationStore1_properties_typeProperties_folderPath": {
			"type": "string",
			"defaultValue": "adflab"
		},
		"sinkRawFiles_Dataset_properties_typeProperties_fileName": {
			"type": "string",
			"defaultValue": ""
		},
		"sinkRawFiles_Dataset_properties_typeProperties_folderPath": {
			"type": "string",
			"defaultValue": "sinkdata/staged_sink"
		},
		"sourceAvailability_Dataset_properties_typeProperties_fileName": {
			"type": "string",
			"defaultValue": "_success"
		},
		"sourceAvailability_Dataset_properties_typeProperties_folderPath": {
			"type": "string",
			"defaultValue": "data/source"
		},
		"sourceFiles_Dataset_properties_typeProperties_fileName": {
			"type": "string",
			"defaultValue": "Product.csv"
		},
		"sourceFiles_Dataset_properties_typeProperties_folderPath": {
			"type": "string",
			"defaultValue": "data/source"
		},
		"OnDemandHDInsightConnection_properties_typeProperties_hostSubscriptionId": {
			"type": "string",
			"defaultValue": "7b68d2b5-dfbe-46e1-938f-98ed143b7953"
		},
		"OnDemandHDInsightConnection_properties_typeProperties_clusterResourceGroup": {
			"type": "string",
			"defaultValue": "7b68d2b5-dfbe-46e1-938f-98ed143b7953"
		},
		"OnDemandHDInsightConnection_properties_typeProperties_servicePrincipalId": {
			"type": "string",
			"defaultValue": "dsf"
		},
		"OnDemandHDInsightConnection_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"hdiOnDemand_ls_properties_typeProperties_hostSubscriptionId": {
			"type": "string",
			"defaultValue": "7b68d2b5-dfbe-46e1-938f-98ed143b7953"
		},
		"hdiOnDemand_ls_properties_typeProperties_clusterResourceGroup": {
			"type": "string",
			"defaultValue": "hdi-ondemand-rg"
		},
		"hdiOnDemand_ls_properties_typeProperties_servicePrincipalId": {
			"type": "string",
			"defaultValue": "9f2d5c32-a9ff-4d84-a60e-a585b275ee0f"
		},
		"hdiOnDemand_ls_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"linkedService1_properties_typeProperties_userName": {
			"type": "string",
			"defaultValue": "asA"
		},
		"linkedService2_properties_typeProperties_hostSubscriptionId": {
			"type": "string",
			"defaultValue": "7b68d2b5-dfbe-46e1-938f-98ed143b7953"
		},
		"linkedService2_properties_typeProperties_clusterResourceGroup": {
			"type": "string",
			"defaultValue": "ADF-BOT"
		},
		"linkedService2_properties_typeProperties_servicePrincipalId": {
			"type": "string",
			"defaultValue": "ewr"
		},
		"linkedService2_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"linkedService2_properties_typeProperties_clusterUserName": {
			"type": "string",
			"defaultValue": "wewe"
		},
		"linkedService2_properties_typeProperties_clusterSshUserName": {
			"type": "string",
			"defaultValue": "as"
		},
		"linkedService_hdinsightBYOC_properties_typeProperties_userName": {
			"type": "string",
			"defaultValue": "admin"
		}
	},
	"variables": {
		"factoryId": "[concat('Microsoft.DataFactory/factories/', parameters('factoryName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('factoryName'), '/AmazonS31')]",
			"type": "Microsoft.DataFactory/factories/linkedServices",
			"apiVersion": "2018-06-01",
			"properties": {
				"annotations": [],
				"type": "AmazonS3",
				"typeProperties": {
					"serviceUrl": "",
					"accessKeyId": "[parameters('AmazonS31_properties_typeProperties_accessKeyId')]",
					"secretAccessKey": {
						"type": "SecureString",
						"value": "[parameters('AmazonS31_secretAccessKey')]"
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/AzureDataLakeStorageGen2_ls')]",
			"type": "Microsoft.DataFactory/factories/linkedServices",
			"apiVersion": "2018-06-01",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorageGen2_ls_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorageGen2_ls_accountKey')]"
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/AzureDatabricks_LS')]",
			"type": "Microsoft.DataFactory/factories/linkedServices",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Linked service to connect  the Azure Databricks workspace. Please modify the properties as required.  ",
				"annotations": [],
				"type": "AzureDatabricks",
				"typeProperties": {
					"domain": "https://eastus.azuredatabricks.net",
					"accessToken": {
						"type": "SecureString",
						"value": "[parameters('AzureDatabricks_LS_accessToken')]"
					},
					"existingClusterId": "0320-025440-yes123"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/AzureDatabricks_jobCluster_LS')]",
			"type": "Microsoft.DataFactory/factories/linkedServices",
			"apiVersion": "2018-06-01",
			"properties": {
				"annotations": [],
				"type": "AzureDatabricks",
				"typeProperties": {
					"domain": "https://eastus.azuredatabricks.net",
					"accessToken": {
						"type": "SecureString",
						"value": "[parameters('AzureDatabricks_jobCluster_LS_accessToken')]"
					},
					"newClusterNodeType": "Standard_DS3_v2",
					"newClusterNumOfWorker": "1:2",
					"newClusterVersion": "5.0.x-scala2.11",
					"newClusterEnableElasticDisk": true
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/hdiOnDemand_storage_ls')]",
			"type": "Microsoft.DataFactory/factories/linkedServices",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Use the same storage account for storing the resource files (scripts, input file) and for creating the HDI on-demand cluster.",
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('hdiOnDemand_storage_ls_connectionString')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/sinkBlob_LS')]",
			"type": "Microsoft.DataFactory/factories/linkedServices",
			"apiVersion": "2018-06-01",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('sinkBlob_LS_connectionString')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/sourceBlob_LS')]",
			"type": "Microsoft.DataFactory/factories/linkedServices",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Source Azure Blob storage account where the source files are.  ",
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"sasUri": "**********?**********"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Chaining outputs')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Until1",
						"type": "Until",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@variables('val')",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "Get ID",
									"type": "WebActivity",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"url": {
											"value": "@concat('https://reqres.in/api/users/',variables('count'))",
											"type": "Expression"
										},
										"method": "GET",
										"headers": {},
										"linkedServices": [],
										"datasets": []
									}
								},
								{
									"name": "Get Item using ID",
									"type": "WebActivity",
									"dependsOn": [
										{
											"activity": "Get ID",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"url": {
											"value": "@concat('https://reqres.in/api/users/',string(add(activity('Get ID').output.data.id,1)))",
											"type": "Expression"
										},
										"method": "GET",
										"headers": {},
										"linkedServices": [],
										"datasets": []
									}
								},
								{
									"name": "increment",
									"type": "SetVariable",
									"dependsOn": [
										{
											"activity": "Get Item using ID",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"variableName": "count",
										"value": {
											"value": "@string(add(activity('Get ID').output.data.id,1))",
											"type": "Expression"
										}
									}
								}
							],
							"timeout": "7.00:00:00"
						}
					}
				],
				"variables": {
					"val": {
						"type": "Boolean",
						"defaultValue": false
					},
					"count": {
						"type": "String",
						"defaultValue": "2"
					}
				},
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/DataDestinationStore1')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Connection to your Azure Data Lake Store.",
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorageGen2_ls",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureBlobFSFile",
				"typeProperties": {
					"format": {
						"type": "TextFormat",
						"columnDelimiter": ",",
						"rowDelimiter": "",
						"nullValue": "\\N",
						"treatEmptyAsNull": true,
						"skipLineCount": 0,
						"firstRowAsHeader": false
					},
					"folderPath": "[parameters('DataDestinationStore1_properties_typeProperties_folderPath')]"
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/linkedServices/AzureDataLakeStorageGen2_ls')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/DataSourceStore1')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Connection your Amazon S3 store.",
				"linkedServiceName": {
					"referenceName": "AmazonS31",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AmazonS3Object",
				"typeProperties": {
					"bucketName": "adflab"
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/linkedServices/AmazonS31')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/sinkRawFiles_Dataset')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Dataset representing files that are copied into the sink/ destination storage. ",
				"linkedServiceName": {
					"referenceName": "sinkBlob_LS",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureBlob",
				"typeProperties": {
					"fileName": "[parameters('sinkRawFiles_Dataset_properties_typeProperties_fileName')]",
					"folderPath": "[parameters('sinkRawFiles_Dataset_properties_typeProperties_folderPath')]"
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/linkedServices/sinkBlob_LS')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/sourceAvailability_Dataset')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Dataset to check if '_success' flag/ file is available in source. If not, then the activity fails, letting you know that source dataset is not ready.\n\nAction: Please create dsdsdsdsdsdfsdfsdf\n\nContext: In Spark/ Hadoop environments, generally users leverage flags to identify if the data has been correctly written through the distributed compute engines. E.g. '_success' defines all nodes have successfully written the data.",
				"linkedServiceName": {
					"referenceName": "sourceBlob_LS",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureBlob",
				"typeProperties": {
					"format": {
						"type": "TextFormat",
						"columnDelimiter": ",",
						"rowDelimiter": "",
						"nullValue": "\\N",
						"treatEmptyAsNull": true,
						"skipLineCount": 0,
						"firstRowAsHeader": false
					},
					"fileName": "[parameters('sourceAvailability_Dataset_properties_typeProperties_fileName')]",
					"folderPath": "[parameters('sourceAvailability_Dataset_properties_typeProperties_folderPath')]"
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/linkedServices/sourceBlob_LS')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/sourceFiles_Dataset')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Dataset pointing to the actual data files in source.",
				"linkedServiceName": {
					"referenceName": "sourceBlob_LS",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureBlob",
				"typeProperties": {
					"fileName": "[parameters('sourceFiles_Dataset_properties_typeProperties_fileName')]",
					"folderPath": "[parameters('sourceFiles_Dataset_properties_typeProperties_folderPath')]"
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/linkedServices/sourceBlob_LS')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/OnDemandHDInsightConnection')]",
			"type": "Microsoft.DataFactory/factories/linkedServices",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "On-demand HDInsight cluster created by ADF for the PySpark application execution. Please ensure you configure an appropriate AAD service principal. In this example, use the same storage account for both on-demand HDI and the resource files for simplicity.",
				"annotations": [],
				"type": "HDInsightOnDemand",
				"typeProperties": {
					"clusterType": "hadoop",
					"clusterSize": 4,
					"timeToLive": "00:05:00",
					"version": "3.6",
					"hostSubscriptionId": "[parameters('OnDemandHDInsightConnection_properties_typeProperties_hostSubscriptionId')]",
					"clusterResourceGroup": "[parameters('OnDemandHDInsightConnection_properties_typeProperties_clusterResourceGroup')]",
					"servicePrincipalId": "[parameters('OnDemandHDInsightConnection_properties_typeProperties_servicePrincipalId')]",
					"tenant": "[parameters('OnDemandHDInsightConnection_properties_typeProperties_tenant')]",
					"clusterNamePrefix": "",
					"clusterPassword": {
						"type": "SecureString",
						"value": "[parameters('OnDemandHDInsightConnection_clusterPassword')]"
					},
					"clusterSshPassword": {
						"type": "SecureString",
						"value": "[parameters('OnDemandHDInsightConnection_clusterSshPassword')]"
					},
					"servicePrincipalKey": {
						"type": "SecureString",
						"value": "[parameters('OnDemandHDInsightConnection_servicePrincipalKey')]"
					},
					"linkedServiceName": {
						"referenceName": "hdiOnDemand_storage_ls",
						"type": "LinkedServiceReference"
					},
					"headNodeSize": "",
					"dataNodeSize": "",
					"zookeeperNodeSize": ""
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/linkedServices/hdiOnDemand_storage_ls')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/hdiOnDemand_ls')]",
			"type": "Microsoft.DataFactory/factories/linkedServices",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "On-demand HDInsight cluster created by ADF for the PySpark application execution. Please ensure you configure an appropriate AAD service principal. In this example, use the same storage account for both on-demand HDI and the resource files for simplicity.",
				"annotations": [],
				"type": "HDInsightOnDemand",
				"typeProperties": {
					"clusterType": "spark",
					"clusterSize": 1,
					"timeToLive": "00:05:00",
					"version": "3.6",
					"hostSubscriptionId": "[parameters('hdiOnDemand_ls_properties_typeProperties_hostSubscriptionId')]",
					"clusterResourceGroup": "[parameters('hdiOnDemand_ls_properties_typeProperties_clusterResourceGroup')]",
					"servicePrincipalId": "[parameters('hdiOnDemand_ls_properties_typeProperties_servicePrincipalId')]",
					"tenant": "[parameters('hdiOnDemand_ls_properties_typeProperties_tenant')]",
					"clusterNamePrefix": "ondemand-cluster-",
					"clusterPassword": {
						"type": "SecureString",
						"value": "[parameters('hdiOnDemand_ls_clusterPassword')]"
					},
					"clusterSshPassword": {
						"type": "SecureString",
						"value": "[parameters('hdiOnDemand_ls_clusterSshPassword')]"
					},
					"sparkVersion": "",
					"servicePrincipalKey": {
						"type": "SecureString",
						"value": "[parameters('hdiOnDemand_ls_servicePrincipalKey')]"
					},
					"additionalLinkedServiceNames": [
						{
							"referenceName": "hdiOnDemand_storage_ls",
							"type": "LinkedServiceReference"
						}
					],
					"linkedServiceName": {
						"referenceName": "hdiOnDemand_storage_ls",
						"type": "LinkedServiceReference"
					},
					"headNodeSize": "",
					"dataNodeSize": "",
					"zookeeperNodeSize": ""
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/linkedServices/hdiOnDemand_storage_ls')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/linkedService1')]",
			"type": "Microsoft.DataFactory/factories/linkedServices",
			"apiVersion": "2018-06-01",
			"properties": {
				"annotations": [],
				"type": "HDInsight",
				"typeProperties": {
					"clusterUri": "https://sparkdemo.azurehdinsight.net",
					"userName": "[parameters('linkedService1_properties_typeProperties_userName')]",
					"password": {
						"type": "SecureString",
						"value": "[parameters('linkedService1_password')]"
					},
					"isEspEnabled": true,
					"linkedServiceName": {
						"referenceName": "hdiOnDemand_storage_ls",
						"type": "LinkedServiceReference"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/linkedServices/hdiOnDemand_storage_ls')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/linkedService2')]",
			"type": "Microsoft.DataFactory/factories/linkedServices",
			"apiVersion": "2018-06-01",
			"properties": {
				"annotations": [],
				"type": "HDInsightOnDemand",
				"typeProperties": {
					"clusterType": "hadoop",
					"clusterSize": 4,
					"timeToLive": "00:05:00",
					"version": "3.6",
					"hostSubscriptionId": "[parameters('linkedService2_properties_typeProperties_hostSubscriptionId')]",
					"clusterResourceGroup": "[parameters('linkedService2_properties_typeProperties_clusterResourceGroup')]",
					"servicePrincipalId": "[parameters('linkedService2_properties_typeProperties_servicePrincipalId')]",
					"osType": "Linux",
					"tenant": "[parameters('linkedService2_properties_typeProperties_tenant')]",
					"clusterNamePrefix": "",
					"clusterUserName": "[parameters('linkedService2_properties_typeProperties_clusterUserName')]",
					"clusterPassword": {
						"type": "SecureString",
						"value": "[parameters('linkedService2_clusterPassword')]"
					},
					"clusterSshUserName": "[parameters('linkedService2_properties_typeProperties_clusterSshUserName')]",
					"clusterSshPassword": {
						"type": "SecureString",
						"value": "[parameters('linkedService2_clusterSshPassword')]"
					},
					"scriptActions": [
						{
							"name": "name",
							"uri": "https://uri",
							"parameters": "param",
							"roles": [
								"headnode",
								"workernode"
							]
						}
					],
					"servicePrincipalKey": {
						"type": "SecureString",
						"value": "[parameters('linkedService2_servicePrincipalKey')]"
					},
					"linkedServiceName": {
						"referenceName": "hdiOnDemand_storage_ls",
						"type": "LinkedServiceReference"
					},
					"headNodeSize": "",
					"dataNodeSize": "",
					"zookeeperNodeSize": ""
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/linkedServices/hdiOnDemand_storage_ls')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/linkedService_hdinsightBYOC')]",
			"type": "Microsoft.DataFactory/factories/linkedServices",
			"apiVersion": "2018-06-01",
			"properties": {
				"annotations": [],
				"type": "HDInsight",
				"typeProperties": {
					"clusterUri": "https://abnarain-byoc.azurehdinsight.net",
					"userName": "[parameters('linkedService_hdinsightBYOC_properties_typeProperties_userName')]",
					"password": {
						"type": "SecureString",
						"value": "[parameters('linkedService_hdinsightBYOC_password')]"
					},
					"isEspEnabled": false,
					"linkedServiceName": {
						"referenceName": "hdiOnDemand_storage_ls",
						"type": "LinkedServiceReference"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/linkedServices/hdiOnDemand_storage_ls')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/CopyData_from_Amazon_S3_to_AzureDataLakeStorage1')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Copy data from Amazon S3 to Azure Data Lake Storage.\n\nYou can also go to \"Copy Data Tool\" to get the pipeline for more connectors and scenarios.",
				"activities": [
					{
						"name": "AmazonS3_to_Azure",
						"description": "Copy data from Amazon S3 to Azure Data Lake Storage.",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "FileSystemSource",
								"recursive": true
							},
							"sink": {
								"type": "AzureBlobFSSink"
							},
							"enableStaging": false,
							"dataIntegrationUnits": 0
						},
						"inputs": [
							{
								"referenceName": "DataSourceStore1",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DataDestinationStore1",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/datasets/DataSourceStore1')]",
				"[concat(variables('factoryId'), '/datasets/DataDestinationStore1')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/ETL with Azure Databricks')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Run a simple ETL job using Azure Databricks, with a single pane of glass monitoring from ADF.\n\nIn the template, we check for source dataset availability. Once it is available we copy it into a blob storage for staging using a Copy activity. The same storage is accessed from Databricks clusters while processing the data (ETL). The output is stored in the same storage under 'output' folder. Various notebook properties are referenced as expressions using pipeline parameters, which lets you configure more generic and reusable pipelines. \n \nFor steps on setting up storage and databricks notebook refer https://aka.ms/databricks-instructions. ",
				"activities": [
					{
						"name": "Availability flag",
						"description": "Lookup (or Get Metadata) activity is used to get information about the source files if they are available for processing. \nIn this template, only when the '_success' flag/ file is available at source, would the downstream activities be triggered. ",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "BlobSource",
								"recursive": true
							},
							"dataset": {
								"referenceName": "sourceAvailability_Dataset",
								"type": "DatasetReference",
								"parameters": {}
							}
						}
					},
					{
						"name": "file-to-blob",
						"description": "Copy activity copies the actual files/ dataset to be processed by Databricks into a staging store. This storage should be accessible by the Azure Databricks cluster referenced in the next activity. ",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Availability flag",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "BlobSource",
								"recursive": true
							},
							"sink": {
								"type": "BlobSink"
							},
							"enableStaging": false,
							"dataIntegrationUnits": 0
						},
						"inputs": [
							{
								"referenceName": "sourceFiles_Dataset",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "sinkRawFiles_Dataset",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "ETL",
						"description": "Databricks Notebook activity does the processing of the data copied in the previous step (copy activity).  Please ensure you have added the databricks notebook (https://adflabstaging1.blob.core.windows.net/share/Transformations.html) in the databricks work-space and referenced it in the notebook activity in ADF.",
						"type": "DatabricksNotebook",
						"dependsOn": [
							{
								"activity": "file-to-blob",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebookPath": "/Shared/Transformations",
							"baseParameters": {
								"input": {
									"value": "@pipeline().parameters.inputPath",
									"type": "Expression"
								},
								"output": {
									"value": "@pipeline().parameters.outputPath",
									"type": "Expression"
								},
								"filename": {
									"value": "@pipeline().parameters.fileName",
									"type": "Expression"
								},
								"pipelineRunId": {
									"value": "@pipeline().RunId",
									"type": "Expression"
								}
							}
						},
						"linkedServiceName": {
							"referenceName": "AzureDatabricks_LS",
							"type": "LinkedServiceReference"
						}
					}
				],
				"parameters": {
					"inputPath": {
						"type": "String",
						"defaultValue": "/staged_sink"
					},
					"outputPath": {
						"type": "String",
						"defaultValue": "/processed_sink"
					},
					"fileName": {
						"type": "String",
						"defaultValue": "Product.csv"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/datasets/sourceAvailability_Dataset')]",
				"[concat(variables('factoryId'), '/datasets/sourceFiles_Dataset')]",
				"[concat(variables('factoryId'), '/datasets/sinkRawFiles_Dataset')]",
				"[concat(variables('factoryId'), '/linkedServices/AzureDatabricks_LS')]"
			]
		}
	]
}