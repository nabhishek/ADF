{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"factoryName": {
			"type": "string",
			"metadata": "Data Factory Name",
			"defaultValue": "demo-lab-abnarain"
		}
	},
	"variables": {
		"factoryId": "[concat('Microsoft.DataFactory/factories/', parameters('factoryName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('factoryName'), '/ETL with Azure Databricks')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Availability flag",
						"description": "Lookup (or Get Metadata) activity is used to get information about the source files if they are available for processing. \nIn this template, only when the '_success' flag/ file is available at source, would the downstream activities be triggered. ",
						"type": "Lookup",
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"typeProperties": {
							"source": {
								"type": "BlobSource",
								"recursive": true
							},
							"dataset": {
								"referenceName": "sourceAvailability_Dataset",
								"type": "DatasetReference"
							}
						}
					},
					{
						"name": "file-to-blob",
						"description": "Copy activity copies the actual files/ dataset to be processed by Databricks into a staging store. This storage should be accessible by the Azure Databricks cluster referenced in the next activity. ",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Availability flag",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"typeProperties": {
							"source": {
								"type": "BlobSource",
								"recursive": true
							},
							"sink": {
								"type": "BlobSink"
							},
							"enableStaging": false,
							"dataIntegrationUnits": 0
						},
						"inputs": [
							{
								"referenceName": "sourceFiles_Dataset",
								"type": "DatasetReference"
							}
						],
						"outputs": [
							{
								"referenceName": "sinkRawFiles_Dataset",
								"type": "DatasetReference"
							}
						]
					},
					{
						"name": "ETL",
						"description": "Databricks Notebook activity does the processing of the data copied in the previous step (copy activity). ",
						"type": "DatabricksNotebook",
						"dependsOn": [
							{
								"activity": "file-to-blob",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"typeProperties": {
							"notebookPath": "/adftutorial/Transformations (update)",
							"baseParameters": {
								"input": "/staged_sink",
								"output": "/processed_sink",
								"filename": "Product.csv",
								"pipelineRunId": {
									"value": "@pipeline().RunId",
									"type": "Expression"
								}
							}
						},
						"linkedServiceName": {
							"referenceName": "AzureDatabricks_LS",
							"type": "LinkedServiceReference"
						}
					}
				]
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/datasets/sourceAvailability_Dataset')]",
				"[concat(variables('factoryId'), '/datasets/sourceFiles_Dataset')]",
				"[concat(variables('factoryId'), '/datasets/sinkRawFiles_Dataset')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/sinkRawFiles_Dataset')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Dataset representing files that are copied into the sink/ destination storage. ",
				"linkedServiceName": {
					"referenceName": "sinkBlob_LS",
					"type": "LinkedServiceReference"
				},
				"type": "AzureBlob",
				"typeProperties": {
					"fileName": "",
					"folderPath": "sinkdata/staged_sink"
				}
			}
		},
		{
			"name": "[concat(parameters('factoryName'), '/sourceAvailability_Dataset')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Dataset to check if '_success' flag/ file is available in source. ",
				"linkedServiceName": {
					"referenceName": "sourceBlob_LS",
					"type": "LinkedServiceReference"
				},
				"type": "AzureBlob",
				"typeProperties": {
					"format": {
						"type": "TextFormat",
						"columnDelimiter": ",",
						"rowDelimiter": "",
						"nullValue": "\\N",
						"treatEmptyAsNull": true,
						"skipLineCount": 0,
						"firstRowAsHeader": false
					},
					"fileName": "_success",
					"folderPath": "data/source"
				}
			}
		},
		{
			"name": "[concat(parameters('factoryName'), '/sourceFiles_Dataset')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Dataset pointing to the actual data files in source.",
				"linkedServiceName": {
					"referenceName": "sourceBlob_LS",
					"type": "LinkedServiceReference"
				},
				"type": "AzureBlob",
				"typeProperties": {
					"fileName": "Product.csv",
					"folderPath": "data/source"
				}
			}
		}
	]
}